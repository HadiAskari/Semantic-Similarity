{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import re\n",
    "import transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "import pickle\n",
    "import urllib.request as requests\n",
    "import json\n",
    "import statistics\n",
    "import torch\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"segmentedinput.pkl\", \"rb\") as pkl_handle:\n",
    "\toutput = pickle.load(pkl_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "        #doc = self.nlp(unicode(text.lower()))\n",
    "        text=text.replace('\\\\n', ' ')\n",
    "        textlist=list(text.split(\" \"))\n",
    "        rez = []\n",
    "        for text1 in textlist:\n",
    "            text1=text1.replace(\"\\n\",\" \")\n",
    "            text1=text1.replace(\">\",\"\")\n",
    "            text1=text1.replace(\"♪♪\",\"♪\")\n",
    "            text1=text1.replace(\"♪\",\"tune.\")\n",
    "            text1=text1.replace(\"(\",\"\")\n",
    "            text1=text1.replace(\")\",\"\")\n",
    "            text1=text1.replace(\"]\",\"\")\n",
    "            text1=text1.replace(\"[\",\"\")\n",
    "            text1=text1.replace(\",\",\"\")\n",
    "            text1=text1.replace(\"?\",\"\")\n",
    "            text1=text1.replace(\"!\",\"\")\n",
    "            text1=text1.replace(\"/\",\"\")\n",
    "            text1=text1.replace(\"\\\\\",\"\")\n",
    "            \n",
    "            rez.append(text1)\n",
    "        text=\" \".join([str(item) for item in rez])\n",
    "        doc = nlp(text.lower())\n",
    "        result = []\n",
    "        for token in doc:\n",
    "            # if token.text in nlp.Defaults.stop_words:\n",
    "            #     continue\n",
    "            # if token.is_punct:\n",
    "            #     continue\n",
    "            # if token.lemma_ == '-PRON-':\n",
    "            #     continue\n",
    "            result.append(token.lemma_)\n",
    "        res=\" \".join(result)\n",
    "        final_res=tokenize.sent_tokenize(res)\n",
    "        return final_res\n",
    "\n",
    "\n",
    "# def remove_punct(text):\n",
    "#         #doc = self.nlp(unicode(text.lower()))\n",
    "#         doc = nlp(text.lower())\n",
    "#         result = []\n",
    "#         for token in doc:\n",
    "#             if token.is_punct:\n",
    "#                 continue\n",
    "#             result.append(token.lemma_)\n",
    "#         return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_process_text(text):\n",
    "    text=text.lower()\n",
    "    splitter=re.compile(r'\\.\\s?')\n",
    "    reslist=splitter.split(text)\n",
    "    rez = []\n",
    "    for res in reslist:\n",
    "        res=res.replace('\\\\n', ' ')\n",
    "        res=res.replace(\">\",\"\")\n",
    "        res=res.replace(\"♪♪\",\"♪\")\n",
    "        res=res.replace(\"♪\",\"tune.\")\n",
    "        res=res.replace(\"(\",\"\")\n",
    "        res=res.replace(\")\",\"\")\n",
    "        res=res.replace(\"]\",\"\")\n",
    "        res=res.replace(\"[\",\"\")\n",
    "        res=res.replace(\",\",\"\")\n",
    "        res=res.replace(\"?\",\"\")\n",
    "        res=res.replace(\"!\",\"\")\n",
    "        res=res.replace(\"/\",\"\")\n",
    "        res=res.replace(\"\\\\\",\"\")\n",
    "        if len(res) > 15:\n",
    "            rez.append(res)\n",
    "    return rez\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_new_process_text(text):\n",
    "    text=text.lower()\n",
    "    splitter=re.compile(r'\\.\\s?')\n",
    "    reslist=splitter.split(text)\n",
    "    rez = []\n",
    "    for res in reslist:\n",
    "        res=res.replace('\\\\n', ' ')\n",
    "        res=res.replace(\">\",\"\")\n",
    "        res=res.replace(\"♪♪\",\"♪\")\n",
    "        res=res.replace(\"♪\",\"tune.\")\n",
    "        res=res.replace(\"(\",\"\")\n",
    "        res=res.replace(\")\",\"\")\n",
    "        res=res.replace(\"]\",\"\")\n",
    "        res=res.replace(\"[\",\"\")\n",
    "        res=res.replace(\",\",\"\")\n",
    "        res=res.replace(\"?\",\"\")\n",
    "        res=res.replace(\"!\",\"\")\n",
    "        res=res.replace(\"/\",\"\")\n",
    "        res=res.replace(\"\\'\",\"\")\n",
    "        res=res.replace(\"\\\"\",\"\")\n",
    "        res=res.replace(\"\\\\\",\"\")\n",
    "        rez.append(res)\n",
    "    final=[]\n",
    "    sent=[]\n",
    "    length=0\n",
    "    for item in rez:\n",
    "        length=length+len(item)\n",
    "        sent.append(item)\n",
    "        if length>100:\n",
    "            length=0\n",
    "            temp=' '.join(sent)\n",
    "            final.append(temp)\n",
    "            sent=[]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterText( doc_ref, posOfInterest = [\"NOUN\", \"VERB\"]):\n",
    "\n",
    "    trash_list = []\n",
    "    for entity in doc_ref.ents:\n",
    "        for en in entity.text.split(\" \"):\n",
    "            trash_list.append(en)\n",
    "\n",
    "    #print(trash_list)\n",
    "\n",
    "    ## add nouns and verbs\n",
    "    new_doc_ref = []\n",
    "    for token in doc_ref:\n",
    "        #print(token.text, token.pos_)\n",
    "        #if token.pos_ in [\"VERB\", \"NOUN\"]:\n",
    "        if token.pos_ in posOfInterest:\n",
    "            if token.text not in trash_list:\n",
    "                #print(token.text)\n",
    "                if len(token.text)>1:\n",
    "                    new_doc_ref.append(token.text.replace(\"\\'\", \"\").replace(\"\\\"\", \"\"))\n",
    "\n",
    "    ## add useful name entities\n",
    "    for entity in doc_ref.ents:\n",
    "        if entity.label_ not in [\"DATE\", \"TIME\", \"PERCENT\", \"MONEY\", \"QUANTITY\", \"ORDINAL\", \"CARDINAL\"]:\n",
    "            if entity.text not in new_doc_ref:\n",
    "                #print(entity.text, entity.label_)\n",
    "                new_doc_ref.append(entity.text.replace(\"\\'\", \"\").replace(\"\\\"\", \"\"))\n",
    "\n",
    "    ## remove single-letter words\n",
    "    for text in new_doc_ref:\n",
    "        if (len(text)<=1):\n",
    "            new_doc_ref.remove(text)\n",
    "\n",
    "    return nlp(\" \".join(new_doc_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['FILE_MAF_20220213T082327Z_GMO_00000000030342_01'][4]['ClosedCaption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' everything was tasting great you son of a bitch no not a bad start i mean not all of it was delicious and what i wanted to eat at that moment', 'but it was really fun to see what everyone did very high the level of the chefs as a whole very high anthonyyeah', 'i thought it was a promising start with a lot of nice surprises you look at their technical abilities and some of them are just really downright impressive', \"for the most part i think a couple of the dishes that didn't work for me it was not failureof technique or execution\", \"it was failure of nerve you mentioned that you liked the claw on sara's dish tom the dish i thought was good\", \"just as a presentation i kinda thought it was neat really anthony i loved that claw i thought that's kind of a bold statement\", 'tells me \"i am not a supermarket chicken \" fair enough and it also speaksof a certain confidence you\\'re lucky we didn\\'t get the beak', \"laughing that's right all right well let's speak specifically about the ones we liked the best tony anthony i thought hung's black chicken with geoduck was really sophisticated\", \"had it all  good flavor inventive use of ingredients pretty flawless tiny criticism of hung's dish there could've been a little bit of color on the plate just for a little bit of a contrast\", \"tre's dish anthony that was a dish that had color really mature use of flavor it was not a classical combination of ingredients by any means but it tasted very classical\", \"it was perfectly executed  it looked good okay well i think we're pretty clear sure on who our favorites are\", \"great i'm gonna go get 'em we'd like to see two of you hung   and tre dramatic music tune. tre i tasted hung's food\", \"i knew he would be in the finals with me it was just right there clear as day and it's like this is gonna be the guy who's gonna be right there next to me through challenge through challenge through challenge\", \"it's just gonna be him i don't feel like anybody else in this competition can outcook me congratulations\", 'the two of you were our favorites in this challenge you can smile judges laugh okay what i want to ask both of you is you know this was about first impressions', 'are you both satisfied i was definitely satisfied i tasted the dish in my head and it came out just the way it did', 'what did you think about the appearance of your dish because the one thing that we noticed was the beigeness', \"i was happy with it not everything needs full color just because the color's not there the food is good\", \" that's the final product you might eat with the eyes first but it's your stomach and it's your mouth is what tastes it\", 'tre tre i was very much in my comfort zone i was just really amazed by the fumet once it came down i felt like \"oh yeah i\\'ve got an edge here', '\" gail it was really an amazing way to incorporate the abalone too did you guys see each other\\'s dishes at all oh man i tasted his and i was just in love', 'i was just like wow to me i mean it just amazed me what he did with that geoduck thanks laughter you know both in season one and in season two the chef who won the first elimination challenge went on to win the title of top chef', 'but no pressure really padma tony i want to stress again how good both contributions were hung your food wasimpeccable', 'if you were missing anything it was color and tre yours was perhaps a little more conservative but i have to sayyou brought together color flavor texture and two very differentmain ingredients in a really impressive way', \"i think the judges   all of us felt the same way ultimately tre it's you thank you congratulations congratulations\", \"i feel awesome i'm glad it's a very big big redemption from the quickfire challenge tre as today's winner you get\", \"  a stack of my collected works all signed with obscene doodles inside and next time you and i arein new york at the same time i'm taking you out for late night yakitori and getting you savage drunk\", \"laughter okay so now i'm gonna have to ask you to send back some of your colleagues you want to know what if this is the end i'd rather have it be for something like this then have it be for something like me putting out some food that is garbage\", \"you know what i'm saying right because you want to know what that bleepcan happen to anybody this guy this guy applause tre they do unfortunately want to talk to a few other people\", 'howie   brian and also clay   dale all muttering good job  congratulations good luck  good luck guys you four have the least favorite dishes in this challenge', \"brian   why do you think you're here i got a little excited when i took the dueling snakes and i made my dishes far too complicated for the amount of time that i had\", \"why the decision to fry them they're fried because i knew that they had the elasticity that they could handle the fry\", 'although the snake actually got a little bit tough on me you say that they held up well against the frying', 'i actually think anything holds up well against frying you could fry my toe and batter it well and it would taste good', 'i love cooking stuff on the bone and i think you missed a perfect opportunity keep it on the bone i scored them very nicely and what i was trying to get was i trying to expose the ribs on them', \"and i spent a little bit too long on doing that and they came out nice but the problem was the ribs didn't come exposed like i had wanted them to\", \"because i was gonna have the dueling ribs up here it tasted great how did it taste and you didn't serve it to us everything was tasting great\", \"it just didn't get i did too much and couldn't get it on the dish only thing worse than a wrong decision is dithering about whether or not it was the wrong decision\", \"right or secondguessing yourself dale tell us why you think you're here first i think my lack of knowledge about the products that i had\", 'my strategy was to come up with something fairly neutral once i kind of   tasted the items to kind of see if i could i guess not just get by', 'gail how did you come up with what you did to the alligator alligator the only thing that i was concerned about is i did not understand the texture of the meat', 'i seared off a few different pieces of meat and i thought cooked all the way through was just i just really did not like the texture', 'keeping it medium rare was a big problem for us it was chewy  and we all agreed on that anthonyyou played it safe', \"you tried to cruise through this challenge without being the worst howie i know i'm here because i did not complete the assignment\", 'what happened i had the frog legs next to the to the dishes when we ran out of time i mean you really cut it close but how did you not get those items on a plate i mean this is crucial', 'the frog legs they were 95% there i just wanted to recrisp them because i just you know what i was focusing more on trying to make sure everything was as perfect as i could make it', 'sure i have a question what is your major malfunction what is my major malfunction i mean what is the first rulein the restaurant business you show up on time', \"what's the second rule of business you have your stuffin the window on time at the same time as everybody else\", \"no not at all am i wrong one plate here i want my food now i want it hot  i want what i ordered that's fair enough\", \"in your book though you know there's a little part that says about ecuadorian line cooks i believeecuadorian laughter oh that's unfair man but wait a minute and what does it say it says you can yell at this person at this person and they'll give it to you when it's wrong\", \"but these particular people they give it to you when it's right you son of a bitch gail oh you're in trouble\", 'well you want it fast and you want it right howie of course i want it fast and i want it right and believe me it kills me that a matter of seconds stands between you know me possibly moving on to the next round or going home', \"those last few seconds got the best of me padma clay why do you think you're here i'm not really sure you know i stand behind my dish\", \"it could've possibly been better obviously tomwhen i checked in on you there was about40 minutes left and you already had the wild boar cooked\", 'right did you think it was cooked correctly at that point of when you served it my intentions was to sear it and parcook it', \"what were you aiming for as far as the doneness of it i was going for more of anot welldone at all i think i should've kept 'em you know two bones or three bones as opposed to cutting 'em so thin\", \"you're serving a wedding party of 150 you parcook your chops wild boar isa beautiful thing why are you parcooking your chops i was playing it too safe\", \"so are you still standing behind your dish i'm standing behind my dish but   just checking i think a good chef can notice his own mistakes at the same time\", 'you keep saying you stand behind the dish but you keep backtracking i tell you the stuffing was really inedible', \"for me it wasn't the stuffing that was the worst part it was the chop anthonythis was nota conceptual problem\", \"this was a problem of fundamentals it's overcookedtoo thin not good okay thank you very much we'll call you all back when we're ready\", 'thank you tom thanks where do we begin you know brian he had the two knockout ingredients he had the snake and the eel', \"i was disappointed to see him fry the rattlesnake but it tasted fine yeah but don't you think he could've fried anything and it would've tasted like that you have these stupendous ingredients like eel and rattlesnake\", \"there was not a single egregious food crime on that plate gail dale's dish alligator needs to be cooked a certain way\", \"alligator can be really tough anthony he found himself in over his head and made some smart decisions i think dale stated very eloquently i think better than i could exactly where he went wrong on this dish and what he could've done better\", \"his taste level's a little suspect here clay i mean i found it interesting that you know his first statement through he stands by his dish\", \"dig a little bit and all of a sudden he's backing off of it and he starts making excuses clay had a good concept\", \"he had an idea and i think that he just couldn't pull the trigger the basic technical skill wasn't there to execute\", 'he pulled the trigger 40 minutes early and he ended up with prison chow at the table my first reaction was \"this is inedible', '\" those chops that was like economy class of you know air cambodia gail yeah yeah i need a punching bag', \"a shot of whiskey  anybody got that the other big issue is howie left something off the plate he should've had time\", 'there was no excuse for not getting that done the most frustrating piece of that is that the risotto he did cook was really tasty', \"yeah  so it makes me think how unfortunate it is because chances are that what he was putting to accompany it would've been tasty too\", \"it was a serious failure well what is the greater crime though leaving something off the plate or clay's dish howie would've been better off putting you know putting the frog legs on the plate\", \"right i think clay would've been better off leaving those damn chops off the plate do you send howie home i look at howie and say based on what he served us he does have a lot of potential and he's clearly i think a better chef than clay\", 'he gave us half of a really good dish he failed it was a failure he rightly is right at the precipice and looking all the way down']\n"
     ]
    }
   ],
   "source": [
    "cc=output['FILE_MAF_20220213T082327Z_GMO_00000000030342_01'][4]['ClosedCaption']\n",
    "cc=new_new_process_text(cc)\n",
    "print(cc)\n",
    "\n",
    "# for i in range(len(cc)):\n",
    "#     doc=nlp(cc[i])\n",
    "#     res=filterText(doc)\n",
    "#     cc[i]=res.text\n",
    "# cc=filterText(cc)\n",
    "# print(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlylabels=df1['Name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('gmo_to_uniqueId.csv')\n",
    "materialID=df['lastRunJob'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#materialID to listindex\n",
    "print(materialID.index('FILE_MAF_20220213T082327Z_GMO_00000000030342_01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['FILE_MAF_20210805T224839Z_GMO_00000000296761_01'][5]['ClosedCaption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481e9d8b3eed4d13b4417ba0e290bbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metalist=[]\n",
    "for id in tqdm(materialID):\n",
    "    segments=[]\n",
    "    for i in range(len(output[id])):\n",
    "        cc=output[id][i]['ClosedCaption']\n",
    "        cc=new_new_process_text(cc)\n",
    "        segments.append(cc)\n",
    "    metalist.append(segments)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed combo\n",
    "metalist=[]\n",
    "for id in tqdm(materialID):\n",
    "    segments=[]\n",
    "    for i in range(len(output[id])):\n",
    "        cc=output[id][i]['ClosedCaption']\n",
    "        cc=process_text(cc)\n",
    "        for i in range(len(cc)):\n",
    "            doc=nlp(cc[i])\n",
    "            res=filterText(doc)\n",
    "            cc[i]=res.text\n",
    "        segments.append(cc)\n",
    "    metalist.append(segments)\n",
    "\n",
    "with open('PreProcessed_segmented_combo.pkl', 'wb') as f:\n",
    "    pickle.dump(metalist, f)\n",
    "\n",
    "with open('PreProcessed_segmented_combo.pkl', 'rb') as f:\n",
    "    newmetalistcombo=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metalist[7582][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('New_New_PreProcessed_segmented.pkl', 'wb') as f:\n",
    "    pickle.dump(metalist, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('New_PreProcessed_segmented.pkl', 'rb') as f:\n",
    "    newmetalist=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newmetalist[3313][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('InitialTargetDatabase.csv')    \n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict=dict(zip(df1['Name'], df1.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text=text.replace(\"(\", \"\")\n",
    "    text=text.replace(\")\", \"\")\n",
    "    text=text.replace(\";\", \"\")\n",
    "    text=text.replace(\",\", \"\")\n",
    "    text=text.replace(\"+\", \"\")\n",
    "    text=text.replace(\".\", \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Combined']=df1['Name'] #+' '+df1['info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Combined'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetlist=df1['Combined'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=targetlist[27]\n",
    "temp=temp.replace('*News business use case needs to be scoped by NBCU - priority TBD', '')\n",
    "targetlist[27]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetlist[20]=\"I like playing Sports\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetlist[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listofepisodes=['FILE_MAF_20220215T074142Z_GMO_00000000368391_01', 'FILE_MAF_20220510T214825Z_GMO_00000000393602_01', 'FILE_MAF_20210805T224425Z_GMO_00000000196552_01', 'FILE_MAF_20220215T035724Z_GMO_00000000332330_01', 'FILE_MAF_20220321T225328Z_GMO_00000000354812_01', 'FILE_MAF_20210805T224839Z_GMO_00000000296761_01', 'FILE_MAF_20220510T231142Z_GMO_00000000398884_01', 'FILE_MAF_20211222T181429Z_GMO_00000000007508_01', 'FILE_MAF_20220213T082327Z_GMO_00000000030342_01', 'FILE_MAF_20210809T200034Z_GMO_00000000171494_01', 'FILE_MAF_20211221T215926Z_GMO_00000000322097_01', 'FILE_MAF_20220215T092524Z_GMO_00000000390298_01', 'FILE_MAF_20210728T172450Z_GMO_00000000001655_01', 'FILE_MAF_20220212T185028Z_GMO_00000000007263_01', 'FILE_MAF_20210807T072641Z_GMO_00000000015474_01', 'FILE_MAF_20210730T000327Z_GMO_00000000346793_01', 'FILE_MAF_20211106T183216Z_GMO_00000000345589_01', 'FILE_MAF_20210729T141453Z_GMO_00000000321307_01', 'FILE_MAF_20210729T092524Z_GMO_00000000044319_01', 'FILE_MAF_20220321T225323Z_GMO_00000000347371_01', 'FILE_MAF_20211105T221830Z_GMO_00000000326366_01', 'FILE_MAF_20210805T225951Z_GMO_00000000282894_01', 'FILE_MAF_20211105T221624Z_GMO_00000000296344_01', 'FILE_MAF_20220214T004901Z_GMO_00000000144043_01', 'FILE_MAF_20211107T155325Z_GMO_00000000344447_01', 'FILE_MAF_20220215T024650Z_GMO_00000000293219_01', 'FILE_MAF_20211103T110009Z_GMO_00000000338789_01', 'FILE_MAF_20220215T084154Z_GMO_00000000370430_01', 'FILE_MAF_20211222T061241Z_GMO_00000000295406_01', 'FILE_MAF_20211104T120706Z_GMO_00000000378652_01', 'FILE_MAF_20210806T004607Z_GMO_00000000318755_01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofepisodes=['FILE_MAF_20220211T205601Z_GMO_00000000001313_01', 'FILE_MAF_20220211T205607Z_GMO_00000000001534_01', 'FILE_MAF_20210727T231031Z_GMO_00000000001577_01' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(materialID.index('FILE_MAF_20220213T082327Z_GMO_00000000030342_01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(listofepisodes[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materialID.index('FILE_MAF_20210729T092524Z_GMO_00000000044319_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmetalist[892][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truthdatadf=pd.read_csv('AdwordLabeledDatabase.csv')\n",
    "truthdatadf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getadword(episode, truthdatadf):\n",
    "    for i in range(len(truthdatadf.lastRunJob)):\n",
    "        if episode == truthdatadf.lastRunJob[i]:\n",
    "            return truthdatadf.adword[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(getadword(ep,truthdatadf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NRS Calculation for all 31 episodes with Top1 Aggregation with only the category of the first episode\n",
    "\n",
    "scores=[]\n",
    "for ep in tqdm(listofepisodes):\n",
    "    index=materialID.index(ep)\n",
    "    groundtruth=getadword(ep,truthdatadf)\n",
    "    if groundtruth in target_dict:\n",
    "            x=target_dict[groundtruth]\n",
    "    print(groundtruth)\n",
    "    print(x)\n",
    "    segmentscore=[]\n",
    "    for i in tqdm(range(len(newmetalist[index]))):\n",
    "        if len(newmetalist[index][i])==0:\n",
    "            continue\n",
    "        emb1=model.encode(newmetalist[index][i]) #, convert_to_tensor=True)\n",
    "        emb2=model.encode(targetlist) #, convert_to_tensor=True)\n",
    "        cos_sim=util.cos_sim(emb1,emb2)\n",
    "\n",
    "        #Add all pairs to a list with their cosine similarity score\n",
    "        MaxAggregation = {}\n",
    "        for i in range(cos_sim.shape[1]):\n",
    "            all_sentence_combinations=[]\n",
    "            for j in range(cos_sim.shape[0]):\n",
    "                all_sentence_combinations.append([cos_sim[j][i], j, i])\n",
    "            all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "            MaxAggregation[i]=all_sentence_combinations[0]\n",
    "        segmentscore.append(MaxAggregation[x][0])\n",
    "                \n",
    "    avg=(sum(segmentscore)/len(segmentscore))\n",
    "    scores.append(avg)\n",
    "    \n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NRS Calculation for all 31 episodes with Top3 Aggregation\n",
    "\n",
    "scores=[]\n",
    "for ep in tqdm(listofepisodes):\n",
    "    index=materialID.index(ep)\n",
    "    segmentscore=[]\n",
    "    for i in tqdm(range(len(newmetalist[index]))):\n",
    "        if len(newmetalist[index][i])==0:\n",
    "            continue\n",
    "        emb1=model.encode(newmetalist[index][i]) #, convert_to_tensor=True)\n",
    "        emb2=model.encode(targetlist) #, convert_to_tensor=True)\n",
    "        cos_sim=util.cos_sim(emb1,emb2)\n",
    "\n",
    "        #Add all pairs to a list with their cosine similarity score\n",
    "        MaxAggregation = {}\n",
    "        for i in range(cos_sim.shape[1]):\n",
    "            all_sentence_combinations=[]\n",
    "            for j in range(cos_sim.shape[0]):\n",
    "                all_sentence_combinations.append([cos_sim[j][i], j, i])\n",
    "            all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "            MaxAggregation[i]=all_sentence_combinations[0:3]\n",
    "\n",
    "        for k,v in MaxAggregation.items():\n",
    "            temp=[]\n",
    "            for item in v:\n",
    "                temp.append(item[0])\n",
    "            avg=(sum(temp)/len(temp))\n",
    "            MaxAggregation[k]=avg\n",
    "        segmentscore.append(MaxAggregation[7])\n",
    "                \n",
    "    avg=(sum(segmentscore)/len(segmentscore))\n",
    "    scores.append(avg)\n",
    "    \n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NRS Calculation for all 31 episodes with Harmonic Mean Aggregation\n",
    "\n",
    "scores=[]\n",
    "for ep in tqdm(listofepisodes):\n",
    "    index=materialID.index(ep)\n",
    "    segmentscore=[]\n",
    "    for i in tqdm(range(len(newmetalist[index]))):\n",
    "        if len(newmetalist[index][i])==0:\n",
    "            continue\n",
    "        emb1=model.encode(newmetalist[index][i]) #, convert_to_tensor=True)\n",
    "        emb2=model.encode(targetlist) #, convert_to_tensor=True)\n",
    "        cos_sim=util.cos_sim(emb1,emb2)\n",
    "\n",
    "        #Add all pairs to a list with their cosine similarity score\n",
    "        MaxAggregation = {}\n",
    "        for i in range(cos_sim.shape[1]):\n",
    "            all_sentence_combinations=[]\n",
    "            for j in range(cos_sim.shape[0]):\n",
    "                all_sentence_combinations.append([cos_sim[j][i], j, i])\n",
    "            all_sentence_combinations = [ele for ele in all_sentence_combinations if ele[0] > 0]\n",
    "            all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "            MaxAggregation[i]=all_sentence_combinations\n",
    "\n",
    "        for k,v in MaxAggregation.items():\n",
    "            temp=[]\n",
    "            for item in v:\n",
    "                temp.append(float(item[0]))\n",
    "        \n",
    "            try:\n",
    "                avg=statistics.harmonic_mean(temp)\n",
    "                MaxAggregation[k]=avg\n",
    "            except:\n",
    "                MaxAggregation[k]=0.0   \n",
    "\n",
    "        segmentscore.append(MaxAggregation[7])\n",
    "                \n",
    "    avg=(sum(segmentscore)/len(segmentscore))\n",
    "    scores.append(avg)\n",
    "    \n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fineart_scores=[0.2245, 0.1833, 0.2931, 0.2493, 0.2636, 0.1637, 0.2745, 0.2742, 0.2656, 0.3080, 0.2045, 0.2471, 0.2224, 0.2096, 0.2055, 0.2002, 0.2200, 0.2369, 0.1651, 0.2034, 0.2404, 0.3072, 0.2457, 0.2943, 0.2634, 0.3653, 0.2236, 0.2990, 0.2745, 0.2344, 0.2733]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores=[0.2245, 0.1833, 0.2931, 0.2493, 0.2636, 0.1637, 0.2745, 0.2742, 0.2656, 0.3080, 0.2045, 0.2471, 0.2224, 0.2096, 0.2055, 0.2002, 0.2200, 0.2369, 0.1651, 0.2034, 0.2404, 0.3072, 0.2457, 0.2943, 0.2634, 0.3653, 0.2236, 0.2990, 0.2745, 0.2344, 0.2733]\n",
    "(sum(scores)-0.4606)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.4606/0.3427"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.4327/0.3615"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.041257133615339475/0.017124404610931003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.4021/0.2073"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topchef=['FILE_MAF_20220213T082327Z_GMO_00000000030342_01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_art=['FILE_MAF_20210805T224839Z_GMO_00000000296761_01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "religion=['FILE_MAF_20220212T185028Z_GMO_00000000007263_01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top Chef aggregation TOP1\n",
    "topscores=[]\n",
    "for ep in tqdm(topchef):\n",
    "    index=materialID.index(ep)\n",
    "    segmentscore=[]\n",
    "    for i in tqdm(range(len(newmetalist[index]))):\n",
    "        if len(newmetalist[index][i])==0:\n",
    "            continue\n",
    "        emb1=model.encode(newmetalist[index][i]) #, convert_to_tensor=True)\n",
    "        emb2=model.encode(targetlist) #, convert_to_tensor=True)\n",
    "        cos_sim=util.cos_sim(emb1,emb2)\n",
    "\n",
    "        #Add all pairs to a list with their cosine similarity score\n",
    "        MaxAggregation = {}\n",
    "        for i in range(cos_sim.shape[1]):\n",
    "            all_sentence_combinations=[]\n",
    "            for j in range(cos_sim.shape[0]):\n",
    "                all_sentence_combinations.append([cos_sim[j][i], j, i])\n",
    "            all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "            MaxAggregation[i]=all_sentence_combinations[0]\n",
    "\n",
    "        segmentscore.append(MaxAggregation[7][0])\n",
    "                \n",
    "    avg=(sum(segmentscore)/len(segmentscore))\n",
    "    topscores.append(avg)\n",
    "print(topscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top Chef aggregation TOP3\n",
    "topchefscores=[]\n",
    "for ep in tqdm(topchef):\n",
    "    index=materialID.index(ep)\n",
    "    segmentscore=[]\n",
    "    for i in tqdm(range(len(newmetalist[index]))):\n",
    "        if len(newmetalist[index][i])==0:\n",
    "            continue\n",
    "        emb1=model.encode(newmetalist[index][i]) #, convert_to_tensor=True)\n",
    "        emb2=model.encode(targetlist) #, convert_to_tensor=True)\n",
    "        cos_sim=util.cos_sim(emb1,emb2)\n",
    "\n",
    "\n",
    "\n",
    "        MaxAggregation = {}\n",
    "        for i in range(cos_sim.shape[1]):\n",
    "            all_sentence_combinations=[]\n",
    "            for j in range(cos_sim.shape[0]):\n",
    "                all_sentence_combinations.append([cos_sim[j][i], j, i])\n",
    "            all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "            MaxAggregation[i]=all_sentence_combinations[0:3]\n",
    "\n",
    "        for k,v in MaxAggregation.items():\n",
    "            temp=[]\n",
    "            for item in v:\n",
    "                temp.append(item[0])\n",
    "            avg=(sum(temp)/len(temp))\n",
    "            MaxAggregation[k]=avg\n",
    "        segmentscore.append(MaxAggregation[7])\n",
    "                \n",
    "    avg=(sum(segmentscore)/len(segmentscore))\n",
    "    topchefscores.append(avg)\n",
    "print(topchefscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top Chef aggregation Harmonic Mean\n",
    "topchefscores=[]\n",
    "for ep in tqdm(topchef):\n",
    "    index=materialID.index(ep)\n",
    "    segmentscore=[]\n",
    "    for i in tqdm(range(len(newmetalist[index]))):\n",
    "        if len(newmetalist[index][i])==0:\n",
    "            continue\n",
    "        emb1=model.encode(newmetalist[index][i]) #, convert_to_tensor=True)\n",
    "        emb2=model.encode(targetlist) #, convert_to_tensor=True)\n",
    "        cos_sim=util.cos_sim(emb1,emb2)\n",
    "\n",
    "\n",
    "\n",
    "        MaxAggregation = {}\n",
    "        for i in range(cos_sim.shape[1]):\n",
    "            all_sentence_combinations=[]\n",
    "            for j in range(cos_sim.shape[0]):\n",
    "                all_sentence_combinations.append([cos_sim[j][i], j, i])\n",
    "            all_sentence_combinations = [ele for ele in all_sentence_combinations if ele[0] > 0]\n",
    "            all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "            MaxAggregation[i]=all_sentence_combinations\n",
    "\n",
    "        for k,v in MaxAggregation.items():\n",
    "            temp=[]\n",
    "            for item in v:\n",
    "                temp.append(float(item[0]))\n",
    "        \n",
    "            avg=statistics.harmonic_mean(temp)\n",
    "            MaxAggregation[k]=avg\n",
    "        segmentscore.append(MaxAggregation[7])\n",
    "                \n",
    "    avg=(sum(segmentscore)/len(segmentscore))\n",
    "    topchefscores.append(avg)\n",
    "print(topchefscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topchef)\n",
    "avg=(sum(topchef)/len(topchef))\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(materialID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "materialID.index('FILE_MAF_20210728T162826Z_GMO_00000000007070_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newmetalist[221][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(range(len(newmetalist[221]))):\n",
    "emb1=model.encode(newmetalist[221][1]) #, convert_to_tensor=True)\n",
    "emb2=model.encode(targetlist) #, convert_to_tensor=True)\n",
    "cos_sim=util.cos_sim(emb1,emb2)\n",
    "\n",
    "#Add all pairs to a list with their cosine similarity score\n",
    "MaxAggregation = {}\n",
    "for i in range(cos_sim.shape[1]):\n",
    "    all_sentence_combinations=[]\n",
    "    for j in range(cos_sim.shape[0]):\n",
    "        all_sentence_combinations.append([cos_sim[j][i], j, i])\n",
    "    all_sentence_combinations = [ele for ele in all_sentence_combinations if ele[0] > 0]\n",
    "    all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "    MaxAggregation[i]=all_sentence_combinations\n",
    "\n",
    "for k,v in MaxAggregation.items():\n",
    "    temp=[]\n",
    "    for item in v:\n",
    "        temp.append(float(item[0]))\n",
    "    print(temp)\n",
    "    avg=statistics.harmonic_mean(temp)\n",
    "    MaxAggregation[k]=avg\n",
    "         \n",
    "#Sort list by the highest cosine similarity score\n",
    "#all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "#all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "#print(all_sentence_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newmetalist[221][1][49])\n",
    "print(targetlist[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_sentence_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MaxAggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'FILE_MAF_20220212T185028Z_GMO_00000000007263_01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_sentence_combinations[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep=['FILE_MAF_20211104T120706Z_GMO_00000000378652_01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# index=materialID.index(ep[0])\n",
    "\n",
    "# emb1=model.encode(newmetalist[index][0]) #, convert_to_tensor=True)\n",
    "cc=process_text(cc)\n",
    "emb1=model.encode(cc)\n",
    "emb2=model.encode(targetlist) #, convert_to_tensor=True)\n",
    "cos_sim=util.cos_sim(emb1,emb2)\n",
    "\n",
    "#Add all pairs to a list with their cosine similarity score\n",
    "all_sentence_combinations_ranked = []\n",
    "for i in range(cos_sim.shape[0]):\n",
    "    for j in range(cos_sim.shape[1]):\n",
    "        all_sentence_combinations_ranked.append([cos_sim[i][j], i, j])\n",
    "\n",
    "#Sort list by the highest cosine similarity score\n",
    "all_sentence_combinations_ranked = sorted(all_sentence_combinations_ranked, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "\n",
    "         \n",
    "#Sort list by the highest cosine similarity score\n",
    "#all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newmetalist[index][0][85])\n",
    "print(targetlist[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en1=model.encode(\"tune\")\n",
    "en2=model.encode(\"Music and Audio\")\n",
    "cos_sim=util.cos_sim(en1,en2)\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f8fa3e926face00369008f823e0a800506eaa9d6c726ba0fbe6c8d0f6896eaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
