{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-12 02:24:57.555363: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import re\n",
    "import transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "import pickle\n",
    "import urllib.request as requests\n",
    "import json\n",
    "import statistics\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "df=pd.read_csv('gmo_to_uniqueId.csv')\n",
    "materialID=df['lastRunJob'].to_list()\n",
    "\n",
    "# with open('PreProcessed_segmented.pkl', 'rb') as f:\n",
    "#     newmetalist=pickle.load(f)\n",
    "\n",
    "with open('New_PreProcessed_segmented.pkl', 'rb') as f:\n",
    "    newnewmetalist=pickle.load(f)\n",
    "\n",
    "sampled_datadf=pd.read_csv(\"Sampled_Adword_Labeled.csv\")\n",
    "\n",
    "#For Normal process with target labels only\n",
    "\n",
    "df1=pd.read_csv('ChildAdwordTargetDatabase.csv')\n",
    "target_dict=dict(zip(df1['Name'], df1.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text=text.lower()\n",
    "    text=text.replace(\"(\", \"\")\n",
    "    text=text.replace(\")\", \"\")\n",
    "    text=text.replace(\";\", \"\")\n",
    "    text=text.replace(\",\", \"\")\n",
    "    text=text.replace(\"+\", \"\")\n",
    "    text=text.replace(\".\", \"\")\n",
    "    text=text.replace(\"&\", \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    #doc = self.nlp(unicode(text.lower()))\n",
    "    text=text.replace('\\\\n', ' ')\n",
    "    doc = nlp(text.lower())\n",
    "    result = []\n",
    "    for token in doc:\n",
    "        if token.text in nlp.Defaults.stop_words:\n",
    "            continue\n",
    "        if token.is_punct:\n",
    "            continue\n",
    "        if token.lemma_ == '-PRON-':\n",
    "            continue\n",
    "        result.append(token.lemma_)\n",
    "    return nlp(\" \".join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Name</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Automotive</td>\n",
       "      <td>Automotive Crossover Hatchback Microcar Off-Ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Books and Literature</td>\n",
       "      <td>Books and Literature Art and Photography Books...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Business and Finance</td>\n",
       "      <td>Business and Finance Business Business Account...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Education</td>\n",
       "      <td>Education College Education Primary Education ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Events and Attractions</td>\n",
       "      <td>Events and Attractions Fashion Events Malls &amp; ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                    Name  \\\n",
       "0           0              Automotive   \n",
       "1           1    Books and Literature   \n",
       "2           2    Business and Finance   \n",
       "3           3               Education   \n",
       "4           4  Events and Attractions   \n",
       "\n",
       "                                                info  \n",
       "0  Automotive Crossover Hatchback Microcar Off-Ro...  \n",
       "1  Books and Literature Art and Photography Books...  \n",
       "2  Business and Finance Business Business Account...  \n",
       "3  Education College Education Primary Education ...  \n",
       "4  Events and Attractions Fashion Events Malls & ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df1['Combined']=df1['Name'] #+' '+df1['info']\n",
    "# df1['Combined1']=df1['Name'] +' '+df1['info']\n",
    "# df1['Combined1'].apply(clean)\n",
    "# df1['Combined1'].apply(process_text)\n",
    "# df1['Combined'].apply(clean)\n",
    "\n",
    "df1['info']=df1['info'].apply(clean)\n",
    "#df1['info']=df1['info'].apply(process_text)\n",
    "targetlist=df1['Name'].to_list()\n",
    "# targetlistcalc=df1['Combined1'].to_list()\n",
    "targetlistchild=df1['info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>materialId</th>\n",
       "      <th>lastRunJob</th>\n",
       "      <th>materialtitle</th>\n",
       "      <th>seriestitle</th>\n",
       "      <th>AssetCC</th>\n",
       "      <th>adword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GMO_00000000001313_01</td>\n",
       "      <td>FILE_MAF_20220211T205601Z_GMO_00000000001313_01</td>\n",
       "      <td>FULL HEARTS (EDITED)</td>\n",
       "      <td>friday night lights</td>\n",
       "      <td>Is something happening\\nbetween you and Riggin...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>GMO_00000000001534_01</td>\n",
       "      <td>FILE_MAF_20220211T205607Z_GMO_00000000001534_01</td>\n",
       "      <td>MAY THE BEST MAN WIN (EDITED)</td>\n",
       "      <td>friday night lights</td>\n",
       "      <td>BE HONEST WITH YOURSELF\\nFOR TWO SECONDS AND T...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>GMO_00000000001577_01</td>\n",
       "      <td>FILE_MAF_20210727T231031Z_GMO_00000000001577_01</td>\n",
       "      <td>GAVIN VOLURE</td>\n",
       "      <td>30 rock</td>\n",
       "      <td>WE'RE AT A PARTY, LEMON,\\nTRY TO LOOSEN UP A L...</td>\n",
       "      <td>Television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>GMO_00000000001585_01</td>\n",
       "      <td>FILE_MAF_20210727T231030Z_GMO_00000000001585_01</td>\n",
       "      <td>DO-OVER</td>\n",
       "      <td>30 rock</td>\n",
       "      <td>[bossa nova music] â™ª LLO, PUSSYCAT! (male voic...</td>\n",
       "      <td>Television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>GMO_00000000001598_01</td>\n",
       "      <td>FILE_MAF_20210727T231031Z_GMO_00000000001598_01</td>\n",
       "      <td>CUTBACKS</td>\n",
       "      <td>30 rock</td>\n",
       "      <td>[cheers] &gt;&gt; WHOO!\\n&gt;&gt; WHOO! &gt;&gt; OKAY, GUYS, I J...</td>\n",
       "      <td>Television</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0             materialId  \\\n",
       "0             3           3  GMO_00000000001313_01   \n",
       "1             6           6  GMO_00000000001534_01   \n",
       "2             7           7  GMO_00000000001577_01   \n",
       "3             8           8  GMO_00000000001585_01   \n",
       "4             9           9  GMO_00000000001598_01   \n",
       "\n",
       "                                        lastRunJob  \\\n",
       "0  FILE_MAF_20220211T205601Z_GMO_00000000001313_01   \n",
       "1  FILE_MAF_20220211T205607Z_GMO_00000000001534_01   \n",
       "2  FILE_MAF_20210727T231031Z_GMO_00000000001577_01   \n",
       "3  FILE_MAF_20210727T231030Z_GMO_00000000001585_01   \n",
       "4  FILE_MAF_20210727T231031Z_GMO_00000000001598_01   \n",
       "\n",
       "                   materialtitle          seriestitle  \\\n",
       "0           FULL HEARTS (EDITED)  friday night lights   \n",
       "1  MAY THE BEST MAN WIN (EDITED)  friday night lights   \n",
       "2                   GAVIN VOLURE              30 rock   \n",
       "3                        DO-OVER              30 rock   \n",
       "4                       CUTBACKS              30 rock   \n",
       "\n",
       "                                             AssetCC      adword  \n",
       "0  Is something happening\\nbetween you and Riggin...      Sports  \n",
       "1  BE HONEST WITH YOURSELF\\nFOR TWO SECONDS AND T...      Sports  \n",
       "2  WE'RE AT A PARTY, LEMON,\\nTRY TO LOOSEN UP A L...  Television  \n",
       "3  [bossa nova music] â™ª LLO, PUSSYCAT! (male voic...  Television  \n",
       "4  [cheers] >> WHOO!\\n>> WHOO! >> OKAY, GUYS, I J...  Television  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truthdatadf=pd.read_csv('AdwordLabeledDatabase.csv')\n",
    "truthdatadf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getadword(episode, truthdatadf):\n",
    "    for i in range(len(truthdatadf.lastRunJob)):\n",
    "        if episode == truthdatadf.lastRunJob[i]:\n",
    "            return truthdatadf.adword[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp=targetlistcalc[27]\n",
    "# temp=temp.replace('*News business use case needs to be scoped by NBCU - priority TBD', '')\n",
    "# targetlistcalc[27]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>materialId</th>\n",
       "      <th>lastRunJob</th>\n",
       "      <th>materialtitle</th>\n",
       "      <th>seriestitle</th>\n",
       "      <th>AssetCC</th>\n",
       "      <th>adword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>665</td>\n",
       "      <td>665</td>\n",
       "      <td>GMO_00000000040741_01</td>\n",
       "      <td>FILE_MAF_20210731T020535Z_GMO_00000000040741_01</td>\n",
       "      <td>I KNEW YOU WHEN</td>\n",
       "      <td>friday night lights</td>\n",
       "      <td>LKS, IS IS SLAMMIN' SAMMY MEADE,\\nFO [radio]\\n...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8244</td>\n",
       "      <td>8244</td>\n",
       "      <td>GMO_00000000370441_01</td>\n",
       "      <td>FILE_MAF_20220215T090159Z_GMO_00000000370441_01</td>\n",
       "      <td>Make Up or Break Up?</td>\n",
       "      <td>total bellas</td>\n",
       "      <td>&gt;&gt; NIKKI: Yeah! Welcome to Paris! Tonight on t...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>678</td>\n",
       "      <td>678</td>\n",
       "      <td>GMO_00000000039091_01</td>\n",
       "      <td>FILE_MAF_20220213T121447Z_GMO_00000000039091_01</td>\n",
       "      <td>HOMECOMING (EDITED)</td>\n",
       "      <td>friday night lights</td>\n",
       "      <td>What's after\\nhigh school, Brian? Me and Mack ...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>777</td>\n",
       "      <td>777</td>\n",
       "      <td>GMO_00000000039094_01</td>\n",
       "      <td>FILE_MAF_20220213T173625Z_GMO_00000000039094_01</td>\n",
       "      <td>NEVERMIND (EDITED)</td>\n",
       "      <td>friday night lights</td>\n",
       "      <td>Here's to God in ten years from now, Street, g...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>679</td>\n",
       "      <td>679</td>\n",
       "      <td>GMO_00000000039095_01</td>\n",
       "      <td>FILE_MAF_20220213T173619Z_GMO_00000000039095_01</td>\n",
       "      <td>WHAT TO DO WHILE YOU'RE WAITING (EDITED)</td>\n",
       "      <td>friday night lights</td>\n",
       "      <td>Dad? You're home! Hello, Brian. Waverly? Damn,...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0             materialId  \\\n",
       "0           665         665  GMO_00000000040741_01   \n",
       "1          8244        8244  GMO_00000000370441_01   \n",
       "2           678         678  GMO_00000000039091_01   \n",
       "3           777         777  GMO_00000000039094_01   \n",
       "4           679         679  GMO_00000000039095_01   \n",
       "\n",
       "                                        lastRunJob  \\\n",
       "0  FILE_MAF_20210731T020535Z_GMO_00000000040741_01   \n",
       "1  FILE_MAF_20220215T090159Z_GMO_00000000370441_01   \n",
       "2  FILE_MAF_20220213T121447Z_GMO_00000000039091_01   \n",
       "3  FILE_MAF_20220213T173625Z_GMO_00000000039094_01   \n",
       "4  FILE_MAF_20220213T173619Z_GMO_00000000039095_01   \n",
       "\n",
       "                              materialtitle          seriestitle  \\\n",
       "0                           I KNEW YOU WHEN  friday night lights   \n",
       "1                      Make Up or Break Up?         total bellas   \n",
       "2                       HOMECOMING (EDITED)  friday night lights   \n",
       "3                        NEVERMIND (EDITED)  friday night lights   \n",
       "4  WHAT TO DO WHILE YOU'RE WAITING (EDITED)  friday night lights   \n",
       "\n",
       "                                             AssetCC  adword  \n",
       "0  LKS, IS IS SLAMMIN' SAMMY MEADE,\\nFO [radio]\\n...  Sports  \n",
       "1  >> NIKKI: Yeah! Welcome to Paris! Tonight on t...  Sports  \n",
       "2  What's after\\nhigh school, Brian? Me and Mack ...  Sports  \n",
       "3  Here's to God in ten years from now, Street, g...  Sports  \n",
       "4  Dad? You're home! Hello, Brian. Waverly? Damn,...  Sports  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_datadf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofepisodes=sampled_datadf['lastRunJob'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listofepisodes=['FILE_MAF_20220211T205601Z_GMO_00000000001313_01', 'FILE_MAF_20220211T205607Z_GMO_00000000001534_01', 'FILE_MAF_20210727T231031Z_GMO_00000000001577_01' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keydict(dataframe, targetlist):\n",
    "    d=defaultdict(list)\n",
    "    for target in targetlist:\n",
    "        minidf=dataframe.loc[dataframe['adword']==target]\n",
    "        minilist=minidf['lastRunJob'].to_list()\n",
    "        d[target]=minilist\n",
    "    \n",
    "    return d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetkey_filelist=keydict(sampled_datadf, targetlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #NRS bulk with top 1 save to file regular\n",
    "# dict_to_save=defaultdict(list)\n",
    "# for target in tqdm(targetlist):\n",
    "#     scores=[]\n",
    "#     for ep in tqdm(listofepisodes):\n",
    "#         index=materialID.index(ep)\n",
    "#         groundtruth=getadword(ep,truthdatadf)\n",
    "#         # if groundtruth in target_dict:\n",
    "#         #         x=target_dict[groundtruth]\n",
    "#         x=target_dict[target]\n",
    "#         #print(groundtruth)\n",
    "#         print(x)\n",
    "#         segmentscore=[]\n",
    "#         for i in range(len(newmetalist[index])):\n",
    "#             if len(newmetalist[index][i])==0:\n",
    "#                 continue\n",
    "#             emb1=model.encode(newmetalist[index][i]) #, convert_to_tensor=True)\n",
    "#             emb2=model.encode(targetlist) #, convert_to_tensor=True)\n",
    "#             cos_sim=util.cos_sim(emb1,emb2)\n",
    "\n",
    "#             #For OG Class\n",
    "#             MaxAggregation = {}\n",
    "#             for i in range(cos_sim.shape[1]):\n",
    "#                 all_sentence_combinations=[]\n",
    "#                 for j in range(cos_sim.shape[0]):\n",
    "#                     all_sentence_combinations.append([cos_sim[j][i], j, i])\n",
    "#                 all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "#                 MaxAggregation[i]=all_sentence_combinations[0]\n",
    "                \n",
    "\n",
    "#             segmentscore.append(MaxAggregation[x][0])\n",
    "\n",
    "            \n",
    "                    \n",
    "#         avg=(sum(segmentscore)/len(segmentscore))\n",
    "#         scores.append(avg)\n",
    "        \n",
    "#     print(scores)\n",
    "#     dict_to_save[target]=scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('NRS_top1_regular.pkl', 'wb') as f:\n",
    "#     pickle.dump(dict_to_save, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('NRS_top1_regular.pkl', 'rb') as f:\n",
    "#     NRS_dict=pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(val):\n",
    "    for key, value in target_dict.items():\n",
    "         if val == value:\n",
    "             return key\n",
    " \n",
    "    return \"key doesn't exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dad=[]\n",
    "for i in range(len(newnewmetalist)):\n",
    "    mom=[]\n",
    "    for j in range(len(newnewmetalist[i])):\n",
    "        if len(newnewmetalist[i][j])==0:\n",
    "            continue\n",
    "        mom.append(newnewmetalist[i][j])\n",
    "    dad.append(mom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' just wanted to tell you that i appreciate you patching things up with him'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dad[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' just wanted to tell you that i appreciate you patching things up with him'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newnewmetalist[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ec16708022441789efce80ecd23994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896b6af4317246b19d44490861af0718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Relevance calculation per episode top1\n",
    "dictofrelevance=copy.deepcopy(targetkey_filelist)\n",
    "for key, value in tqdm(targetkey_filelist.items()): #for each IAB Label\n",
    "    if len(targetkey_filelist[key])==0:\n",
    "        continue\n",
    "    for i in tqdm(range(len(targetkey_filelist[key]))): #for each File\n",
    "        index=materialID.index(targetkey_filelist[key][i])\n",
    "        x=target_dict[key] #Index of the target label\n",
    "        MaxAggregation=defaultdict(list)\n",
    "\n",
    "        for l in range(len(dad[index])): #For each Segment\n",
    "            # print(len(newmetalist[index]))\n",
    "            if len(dad[index][l])==0: \n",
    "                continue\n",
    "            emb1=model.encode(dad[index][l]) #, convert_to_tensor=True)\n",
    "            emb2=model.encode(targetlistchild) #, convert_to_tensor=True)\n",
    "            cos_sim=util.cos_sim(emb1,emb2)\n",
    "\n",
    "        #Add all pairs to a list with their cosine similarity score\n",
    "        \n",
    "            for k in range(cos_sim.shape[1]):\n",
    "                all_sentence_combinations=[]\n",
    "                for j in range(cos_sim.shape[0]):\n",
    "                    all_sentence_combinations.append([cos_sim[j][k], j, k])\n",
    "                all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "                MaxAggregation[k].append(all_sentence_combinations[0])\n",
    "    \n",
    "     \n",
    "        dictofrelevance[key][i]={dictofrelevance[key][i]:MaxAggregation}\n",
    "     \n",
    "\n",
    "with open('Relevance_Top1_ChildAdword.pkl', 'wb') as f:\n",
    "    pickle.dump(dictofrelevance, f)   \n",
    " \n",
    "        #RS and NRS for OG Class\n",
    "\n",
    "        # RSlist=MaxAggregation[x]\n",
    "        # score=[]\n",
    "        # for s in RSlist:\n",
    "        #     score.append(s[0])\n",
    "\n",
    "        # rs=sum(score)/len(score)\n",
    "\n",
    "        # NRSlist=NRS_dict[key]\n",
    "        # nrsdenom=sum(NRSlist)-rs/len(NRSlist)\n",
    "\n",
    "        # NRS=rs/nrsdenom\n",
    "\n",
    "        \n",
    "        # #RS and ClassPredicted for Predicted Class\n",
    "\n",
    "        # order=[]\n",
    "        # for key in MaxAggregation.keys():\n",
    "        #     l=MaxAggregation[key]\n",
    "        #     summ=[]\n",
    "        #     for val in l:\n",
    "        #         summ.append(val[0])\n",
    "        #     order.append(sum(summ))\n",
    "        \n",
    "        \n",
    "        # max_val=max(order)\n",
    "        # max_index=order.index(max_val)\n",
    "        # ClassPredicted=get_key(max_index)\n",
    "\n",
    "        # RSlistss=MaxAggregation[max_index]\n",
    "        # scoress=[]\n",
    "        # for s in RSlistss:\n",
    "        #     scoress.append(s[0])\n",
    "\n",
    "        # RS_CP=sum(scoress)/len(scoress)\n",
    "\n",
    "        # d={targetkey_filelist[key][i]:{'NRS': NRS, \"Class_Predicted\": ClassPredicted, \"RS_CP\": RS_CP, \"RS_OG\": rs }}\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relevance calculation per episode top3\n",
    "dictofrelevance=copy.deepcopy(targetkey_filelist)\n",
    "for key, value in tqdm(targetkey_filelist.items()): #for each IAB Label\n",
    "    if len(targetkey_filelist[key])==0:\n",
    "        continue\n",
    "    for i in tqdm(range(len(targetkey_filelist[key]))): #for each File\n",
    "        index=materialID.index(targetkey_filelist[key][i])\n",
    "        x=target_dict[key] #Index of the target label\n",
    "        MaxAggregation=defaultdict(list)\n",
    "\n",
    "        for l in range(len(newmetalist[index])): #For each Segment\n",
    "            # print(len(newmetalist[index]))\n",
    "            if len(newmetalist[index][l])==0: \n",
    "                continue\n",
    "            emb1=model.encode(newmetalist[index][l]) #, convert_to_tensor=True)\n",
    "            emb2=model.encode(targetlist) #, convert_to_tensor=True)\n",
    "            cos_sim=util.cos_sim(emb1,emb2)\n",
    "\n",
    "        #Add all pairs to a list with their cosine similarity score\n",
    "        \n",
    "            for k in range(cos_sim.shape[1]):\n",
    "                all_sentence_combinations=[]\n",
    "                for j in range(cos_sim.shape[0]):\n",
    "                    all_sentence_combinations.append([cos_sim[j][k], j, k])\n",
    "                all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "                templist=all_sentence_combinations[0:3]\n",
    "                temp=[]\n",
    "                sent=[]\n",
    "                for item in templist:\n",
    "                    temp.append(item[0])\n",
    "                    sent.append(list([item[1],item[2]]))\n",
    "                avg=(sum(temp)/len(temp))\n",
    "                comb=list([avg, sent])\n",
    "                MaxAggregation[k].append(comb)\n",
    "            \n",
    "            # print(MaxAggregation)\n",
    "            \n",
    "    \n",
    "        # print(MaxAggregation)\n",
    "        # print(dictofrelevance[key][i])\n",
    "        dictofrelevance[key][i]={dictofrelevance[key][i]:MaxAggregation}\n",
    "        # print(dictofrelevance[key][i])\n",
    "\n",
    "with open('Relevance_AdditionalSignal_top3_regular.pkl', 'wb') as f:\n",
    "    pickle.dump(dictofrelevance, f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relevance calculation per episode Harmonic\n",
    "dictofrelevance=copy.deepcopy(targetkey_filelist)\n",
    "for key, value in tqdm(targetkey_filelist.items()): #for each IAB Label\n",
    "    if len(targetkey_filelist[key])==0:\n",
    "        continue\n",
    "    for i in tqdm(range(len(targetkey_filelist[key]))): #for each File\n",
    "        index=materialID.index(targetkey_filelist[key][i])\n",
    "        x=target_dict[key] #Index of the target label\n",
    "        MaxAggregation=defaultdict(list)\n",
    "\n",
    "        for l in range(len(newmetalist[index])): #For each Segment\n",
    "            # print(len(newmetalist[index]))\n",
    "            if len(newmetalist[index][l])==0: \n",
    "                continue\n",
    "            emb1=model.encode(newmetalist[index][l]) #, convert_to_tensor=True)\n",
    "            emb2=model.encode(targetlist) #, convert_to_tensor=True)\n",
    "            cos_sim=util.cos_sim(emb1,emb2)\n",
    "\n",
    "        #Add all pairs to a list with their cosine similarity score\n",
    "        \n",
    "            for k in range(cos_sim.shape[1]):\n",
    "                all_sentence_combinations=[]\n",
    "                for j in range(cos_sim.shape[0]):\n",
    "                    all_sentence_combinations.append([cos_sim[j][k], j, k])\n",
    "                all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "                all_sentence_combinations = [ele for ele in all_sentence_combinations if ele[0] > 0]\n",
    "                templist=all_sentence_combinations\n",
    "                temp=[]\n",
    "                sent=[]\n",
    "                for item in templist:\n",
    "                    temp.append(float(item[0]))\n",
    "                try:\n",
    "                    sent.append([templist[0][1],templist[0][2],templist[0][0]])\n",
    "                    avg=statistics.harmonic_mean(temp)\n",
    "                    comb=list([avg, sent])\n",
    "                    MaxAggregation[k].append(comb)\n",
    "                except:\n",
    "                    sent.append(['-'])\n",
    "                    avg=0\n",
    "                    comb=list([avg, sent])\n",
    "                    MaxAggregation[k].append(comb)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            # print(MaxAggregation)\n",
    "            \n",
    "    \n",
    "        # print(MaxAggregation)\n",
    "        # print(dictofrelevance[key][i])\n",
    "        dictofrelevance[key][i]={dictofrelevance[key][i]:MaxAggregation}\n",
    "        # print(dictofrelevance[key][i])\n",
    "\n",
    "with open('Relevance_Harmonic_regular.pkl', 'wb') as f:\n",
    "    pickle.dump(dictofrelevance, f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Relevance_Top1_ChildAdword.pkl', 'rb') as f:\n",
    "     Relevance_dict=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Relevance_dict['Automotive'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Def Top1 Agg\n",
    "def segmentcombiner(dic): #for segments to final dic of relevance scores\n",
    "    dic1=copy.deepcopy(dic)\n",
    "    filename=list(dic1.keys())[0]\n",
    "    defdict=dic1[filename]\n",
    "    for k,v in defdict.items():\n",
    "        RSlist=defdict[k]\n",
    "        score=[]\n",
    "        for s in RSlist:\n",
    "            score.append(s[0])\n",
    "\n",
    "        rs=sum(score)/len(score)\n",
    "        defdict[k]=rs\n",
    "    d={filename:defdict}\n",
    "    return d\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Def Top3 and Harm Agg\n",
    "# def segmentcombinertop3Harm(dic): #for segments to final dic of relevance scores\n",
    "#     dic1=copy.deepcopy(dic)\n",
    "#     filename=list(dic1.keys())[0]\n",
    "#     defdict=dic1[filename]\n",
    "#     for k,v in defdict.items():\n",
    "#         RSlist=defdict[k][0]\n",
    "#         score=[]\n",
    "#         for s in RSlist:\n",
    "#             score.append(s[0])\n",
    "\n",
    "#         rs=sum(score)/len(score)\n",
    "#         defdict[k]=rs\n",
    "#     d={filename:defdict}\n",
    "#     return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(segmentcombiner(Relevance_dict_top3['Automotive'][18]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Relevance_regular.pkl', 'rb') as f:\n",
    "#      Relevance_dict=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e669aa16ec44f9ab7f8b6fcfec5fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcae2e8799a34a59b3059162fbf9d74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47d29ede4844d8384d99999ae531e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8bda427d3745c9ba9fbfffb4790e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97294ccf1bd44069b45cecf82910f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163c5aaf6d5647c3bd1d24e489f8fead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df15c4bbae64e56a4b5009386dc3cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8df6e19f3e4288b83f5ced3d925168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89fe48b813db479f824d79454fdaec23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489b2ab180fc4193b674ca94c309ce1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ea28b88e19430d926bb8a09882d12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e72b39455e4fc5b73f15e598e6b36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5061e075e3c4ade81c14d0c55be5d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b50f69f56bc444292a4f408e4a0eeed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5be3caf12e469ba68832b61760d924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7b0059615f4cf096ad70e0d59a95c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c66eae637f4b8a855d622443e84258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6781bd98ba4de8ad6a70bb1ff4f4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9db3ca8459c435492c8a4047067e510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87723e1e1e1499da920f3c7f79eac3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7c835d5ab24d3e8a0acb5d4b0c5f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Automotive': 1.1185518199181357, 'Books and Literature': 1.1806155004121979, 'Business and Finance': 1.333218089144164, 'Education': 1.1651432565277338, 'Events and Attractions': 1.1272685094122596, 'Family and Relationships': 1.0931180301453156, 'Food & Drink': 1.6569397836646746, 'Hobbies & Interests': 1.0367267053895628, 'Medical Health': 1.6891036298829092, 'Music and Audio': 1.0987961579827805, 'Pets': 0.9013374717644428, 'Real Estate': 2.84922916149329, 'Religion & Spirituality': 1.2678996187919302, 'Science': 1.1590756019772757, 'Shopping': 1.1895382431765187, 'Sports': 1.1372730819655776, 'Style & Fashion': 1.6912651546258568, 'Technology & Computing': 1.7057414794374346, 'Television': 1.339820876652496, 'Travel': 1.3563558891567618}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(NRS(Relevance_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Def NRS for top1 \n",
    "def NRS(dict):\n",
    "    d={}\n",
    "    for k,v in tqdm(dict.items()):#for each label\n",
    "        if len(dict[k]) == 0:\n",
    "            continue\n",
    "        d1={}\n",
    "        for target in tqdm(targetlist): #for each topic\n",
    "            scoreforfiles=[]\n",
    "            if len(dict[target]) == 0:\n",
    "                continue\n",
    "            for i in range(len(dict[k])): #for each file in label\n",
    "                x=target_dict[target]\n",
    "                combinedsegments=segmentcombiner(dict[k][i])\n",
    "                key=list(combinedsegments.keys())[0]\n",
    "                scoreforfiles.append(float(combinedsegments[key][x])) #combine segments \n",
    "            avg=sum(scoreforfiles)/len(scoreforfiles)\n",
    "            d1[target]=avg\n",
    "        d[k]=d1\n",
    "\n",
    "    finaldict={}\n",
    "    for k,v in d.items():\n",
    "        numerator=d[k][k]\n",
    "        denominator=[]\n",
    "        for kk,vv in d.items():\n",
    "            denominator.append(d[kk][k])\n",
    "        avgdenominator=(sum(denominator)-numerator)/(len(denominator)-1)\n",
    "        NRS=numerator/avgdenominator\n",
    "        finaldict[k]=NRS\n",
    "    print(finaldict)\n",
    "\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(RSMetrics(Relevance_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Def RS at 0.15, 0.20, 0.35 and Mean RS \n",
    "def RSMetrics(dict):\n",
    "    d={}\n",
    "    for k,v in tqdm(dict.items()): #for each label\n",
    "        if len(dict[k]) == 0:\n",
    "            continue\n",
    "        scoreforfiles=[]\n",
    "        for i in range(len(dict[k])): #for each file in label\n",
    "            x=target_dict[k]\n",
    "            combinedsegments=segmentcombiner(dict[k][i])\n",
    "            key=list(combinedsegments.keys())[0]\n",
    "            scoreforfiles.append(round(float(combinedsegments[key][x]),2))\n",
    "        rs15=[]\n",
    "        rs20=[]\n",
    "        rs35=[]\n",
    "        for item in scoreforfiles:\n",
    "            if item >= 0.15:\n",
    "                rs15.append(item)\n",
    "            if item >= 0.20:\n",
    "                rs20.append(item)\n",
    "            if item >= 0.35:\n",
    "                rs35.append(item)\n",
    "\n",
    "        RS15=(len(rs15)/len(scoreforfiles))*100\n",
    "        RS20=(len(rs20)/len(scoreforfiles))*100\n",
    "        RS35=(len(rs35)/len(scoreforfiles))*100\n",
    "        d1={\"RS15\":RS15, \"RS20\": RS20, \"RS35\": RS35}\n",
    "        d[k]=d1\n",
    "    \n",
    "    return d\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Def RS at 0.15, 0.20, 0.35 and Mean RS \n",
    "def RSMetricsHM(dict):\n",
    "    d={}\n",
    "    for k,v in tqdm(dict.items()): #for each label\n",
    "        if len(dict[k]) == 0:\n",
    "            continue\n",
    "        scoreforfiles=[]\n",
    "        for i in range(len(dict[k])): #for each file in label\n",
    "            x=target_dict[k]\n",
    "            combinedsegments=segmentcombiner(dict[k][i])\n",
    "            key=list(combinedsegments.keys())[0]\n",
    "            ll=(dict[k][i][key][x])\n",
    "            lll=[]\n",
    "            for score in ll:\n",
    "                print(score[1])\n",
    "                lll.append(score[1][0][2])\n",
    "            avg=sum(lll)/len(ll)\n",
    "            scoreforfiles.append(round(float(avg),2))\n",
    "        rs15=[]\n",
    "        rs20=[]\n",
    "        rs35=[]\n",
    "        for item in scoreforfiles:\n",
    "            if item >= 0.15:\n",
    "                rs15.append(item)\n",
    "            if item >= 0.20:\n",
    "                rs20.append(item)\n",
    "            if item >= 0.35:\n",
    "                rs35.append(item)\n",
    "\n",
    "        RS15=(len(rs15)/len(scoreforfiles))*100\n",
    "        RS20=(len(rs20)/len(scoreforfiles))*100\n",
    "        RS35=(len(rs35)/len(scoreforfiles))*100\n",
    "        d1={\"RS15\":RS15, \"RS20\": RS20, \"RS35\": RS35}\n",
    "        d[k]=d1\n",
    "    \n",
    "    return d\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Prediction(Relevance_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Def Predicted Class and RS of Class\n",
    "def Prediction(dict):\n",
    "    d={}\n",
    "    for k,v in tqdm(dict.items()): #for each label\n",
    "        if len(dict[k]) == 0:\n",
    "            continue\n",
    "        d1={}\n",
    "        for i in range(len(dict[k])): #for each file in label\n",
    "            combinedsegments=segmentcombiner(dict[k][i])\n",
    "            key=list(combinedsegments.keys())[0]\n",
    "            order=[]\n",
    "            for kk in combinedsegments[key].keys():\n",
    "                l=combinedsegments[key][kk]\n",
    "                order.append(l)\n",
    "            max_val=max(order)\n",
    "            max_index=order.index(max_val)\n",
    "            ClassPredicted=get_key(max_index)\n",
    "            RS_CP=combinedsegments[key][max_index]\n",
    "            d2={'ClassPredicted': ClassPredicted, 'RS_CP': RS_CP }\n",
    "            d1[key]=d2\n",
    "        d[k]=d1\n",
    "\n",
    "    return d\n",
    "\n",
    "            # \n",
    "            # scoreforfiles.append(float(combinedsegments[key][x]0))\n",
    "\n",
    "            # order=[]\n",
    "            # for key in MaxAggregation.keys():\n",
    "            #     l=MaxAggregation[key]\n",
    "            #     summ=[]\n",
    "            #     for val in l:\n",
    "            #         summ.append(val[0])\n",
    "            #     order.append(sum(summ))\n",
    "            \n",
    "            \n",
    "            # max_val=max(order)\n",
    "            # max_index=order.index(max_val)\n",
    "            # ClassPredicted=get_key(max_index)\n",
    "\n",
    "            # RSlistss=MaxAggregation[max_index]\n",
    "            # scoress=[]\n",
    "            # for s in RSlistss:\n",
    "            #     scoress.append(s[0])\n",
    "\n",
    "            # RS_CP=sum(scoress)/len(scoress)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Def Predicted Class and RS of Class HM\n",
    "def PredictionHM(dict):\n",
    "    d={}\n",
    "    for k,v in tqdm(dict.items()): #for each label\n",
    "        if len(dict[k]) == 0:\n",
    "            continue\n",
    "        d1={}\n",
    "        for i in range(len(dict[k])): #for each file in label\n",
    "            combinedsegments=segmentcombiner(dict[k][i])\n",
    "            key=list(combinedsegments.keys())[0]\n",
    "            order=[]\n",
    "            for kk in combinedsegments[key].keys():\n",
    "                l=combinedsegments[key][kk]\n",
    "                order.append(l)\n",
    "            max_val=max(order)\n",
    "            max_index=order.index(max_val)\n",
    "            ClassPredicted=get_key(max_index)\n",
    "\n",
    "            ll=(dict[k][i][key][max_index])\n",
    "            lll=[]\n",
    "            for score in ll:\n",
    "                lll.append(score[1][0][2])\n",
    "            avg=sum(lll)/len(ll)\n",
    "\n",
    "\n",
    "            RS_CP=float(avg)\n",
    "            d2={'ClassPredicted': ClassPredicted, 'RS_CP': RS_CP }\n",
    "            d1[key]=d2\n",
    "        d[k]=d1\n",
    "\n",
    "    return d\n",
    "\n",
    "            # \n",
    "            # scoreforfiles.append(float(combinedsegments[key][x]0))\n",
    "\n",
    "            # order=[]\n",
    "            # for key in MaxAggregation.keys():\n",
    "            #     l=MaxAggregation[key]\n",
    "            #     summ=[]\n",
    "            #     for val in l:\n",
    "            #         summ.append(val[0])\n",
    "            #     order.append(sum(summ))\n",
    "            \n",
    "            \n",
    "            # max_val=max(order)\n",
    "            # max_index=order.index(max_val)\n",
    "            # ClassPredicted=get_key(max_index)\n",
    "\n",
    "            # RSlistss=MaxAggregation[max_index]\n",
    "            # scoress=[]\n",
    "            # for s in RSlistss:\n",
    "            #     scoress.append(s[0])\n",
    "\n",
    "            # RS_CP=sum(scoress)/len(scoress)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb86eeef659344fab340af63dbf1c1ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'Automotive': 5.0, 'Books and Literature': 5.0, 'Business and Finance': 10.0, 'Education': 0.0, 'Events and Attractions': 40.0, 'Family and Relationships': 10.0, 'Food & Drink': 65.0, 'Hobbies & Interests': 0.0, 'Medical Health': 70.0, 'Music and Audio': 60.0, 'Pets': 0.0, 'Real Estate': 85.0, 'Religion & Spirituality': 0.0, 'Science': 0.0, 'Shopping': 5.0, 'Sports': 0.0, 'Style & Fashion': 100.0, 'Technology & Computing': 10.0, 'Television': 5.0, 'Travel': 10.0}, 24.0)\n"
     ]
    }
   ],
   "source": [
    "print(mAP(Relevance_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Def mAP\n",
    "def mAP(dic):\n",
    "    res=Prediction(dic)\n",
    "    d={}\n",
    "    mAP=[]\n",
    "    for k,v in res.items():\n",
    "        classes=[]\n",
    "        truth=[]\n",
    "\n",
    "        for kk,vv in res[k].items():\n",
    "            ClassPred=res[k][kk]['ClassPredicted']\n",
    "            if ClassPred==k:\n",
    "                classes.append(ClassPred)\n",
    "            truth.append(k)\n",
    "        precision=(len(classes)/len(truth))*100\n",
    "        mAP.append(precision)\n",
    "        d[k]=precision\n",
    "    mAPfinal=sum(mAP)/len(mAP)\n",
    "    return(d,mAPfinal)\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Relevance_dict['Automotive'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topksentences(dict):\n",
    "    d=defaultdict(list)\n",
    "    adwords=[]\n",
    "    texts=[]\n",
    "    scores=[]\n",
    "    for k,v in tqdm(dict.items()):#for each label\n",
    "        if len(dict[k]) == 0:\n",
    "            continue\n",
    "        scoreforfiles=[]\n",
    " \n",
    "        for i in range(len(dict[k])): #for each file in label\n",
    "            x=target_dict[k]\n",
    "            seg_rel=copy.deepcopy(dict[k][i])\n",
    "            filename=list(seg_rel.keys())[0]\n",
    "            defdict=seg_rel[filename]\n",
    "            segments=defdict[x]\n",
    "            # for count,seg1 in enumerate(segments):\n",
    "            #     if len(seg1)==0:\n",
    "            #         segments[count].append('Buffer')\n",
    "\n",
    "\n",
    "            for count,seg in enumerate(segments):\n",
    "                seg.append(filename)\n",
    "                seg.append(count)                \n",
    "                scoreforfiles.append(seg)\n",
    "        sort=sorted(scoreforfiles, key=lambda x: x[0], reverse=True)\n",
    "        Final=sort[0:20]\n",
    "        # print(Final)\n",
    "        \n",
    "        for items in Final:\n",
    "            index=materialID.index(items[3])\n",
    "            try:\n",
    "                items.append(dad[index][items[4]][items[1]])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(index,items[4],items[1])\n",
    "                # print(dad[index][items[4]][items[1]])\n",
    "                print(newnewmetalist[index][items[4]][items[1]])\n",
    "                # items.append(dad[index][items[4]][items[1]])\n",
    "                continue\n",
    "        # print(Final)\n",
    "        ff=[]\n",
    "        for f in Final:\n",
    "            dic={}\n",
    "            temp=str(f[0])\n",
    "            temp=temp.replace('tensor(', \"\")\n",
    "            temp=temp.replace(')', \"\")\n",
    "            dic['Adword']=k\n",
    "            dic['Text']=f[5]\n",
    "            dic['Score']=temp\n",
    "            \n",
    "            fff.append(dic)\n",
    "    \n",
    "        # d[k].extend(ff)\n",
    "    return fff\n",
    "    \n",
    "        \n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(materialID.index('FILE_MAF_20210729T074455Z_GMO_00000000044309_01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2d59d9d1944bf3a279c157d19108f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dict_top20=topksentences(Relevance_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'DataFrame' has no attribute 'from_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/mnt/data0/haskari/Comcast/Testing_NRS_Top1_Regular.ipynb Cell 48'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsohrab.idav.ucdavis.edu/mnt/data0/haskari/Comcast/Testing_NRS_Top1_Regular.ipynb#ch0000047vscode-remote?line=0'>1</a>\u001b[0m df11\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39;49mDataFrame\u001b[39m.\u001b[39;49mfrom_list(dict_top20)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'DataFrame' has no attribute 'from_list'"
     ]
    }
   ],
   "source": [
    "df11=pd.DataFrame.from_list(dict_top20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Automotive</th>\n",
       "      <th>Books and Literature</th>\n",
       "      <th>Business and Finance</th>\n",
       "      <th>Education</th>\n",
       "      <th>Events and Attractions</th>\n",
       "      <th>Family and Relationships</th>\n",
       "      <th>Food &amp; Drink</th>\n",
       "      <th>Hobbies &amp; Interests</th>\n",
       "      <th>Medical Health</th>\n",
       "      <th>Music and Audio</th>\n",
       "      <th>Pets</th>\n",
       "      <th>Real Estate</th>\n",
       "      <th>Religion &amp; Spirituality</th>\n",
       "      <th>Science</th>\n",
       "      <th>Shopping</th>\n",
       "      <th>Sports</th>\n",
       "      <th>Style &amp; Fashion</th>\n",
       "      <th>Technology &amp; Computing</th>\n",
       "      <th>Television</th>\n",
       "      <th>Travel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'Adword': 'Automotive', 'Text': 'the most suc...</td>\n",
       "      <td>{'Adword': 'Books and Literature', 'Text': 'fl...</td>\n",
       "      <td>{'Adword': 'Business and Finance', 'Text': 'an...</td>\n",
       "      <td>{'Adword': 'Education', 'Text': 'the grades th...</td>\n",
       "      <td>{'Adword': 'Events and Attractions', 'Text': '...</td>\n",
       "      <td>{'Adword': 'Family and Relationships', 'Text':...</td>\n",
       "      <td>{'Adword': 'Food &amp; Drink', 'Text': 'friday's e...</td>\n",
       "      <td>{'Adword': 'Hobbies &amp; Interests', 'Text': 'any...</td>\n",
       "      <td>{'Adword': 'Medical Health', 'Text': 'there ar...</td>\n",
       "      <td>{'Adword': 'Music and Audio', 'Text': 'anythin...</td>\n",
       "      <td>{'Adword': 'Pets', 'Text': 'my spare set of ca...</td>\n",
       "      <td>{'Adword': 'Real Estate', 'Text': '  as the ma...</td>\n",
       "      <td>{'Adword': 'Religion &amp; Spirituality', 'Text': ...</td>\n",
       "      <td>{'Adword': 'Science', 'Text': 'chandra the wor...</td>\n",
       "      <td>{'Adword': 'Shopping', 'Text': ' anything in t...</td>\n",
       "      <td>{'Adword': 'Sports', 'Text': 'talk about a who...</td>\n",
       "      <td>{'Adword': 'Style &amp; Fashion', 'Text': ''s\" fas...</td>\n",
       "      <td>{'Adword': 'Technology &amp; Computing', 'Text': '...</td>\n",
       "      <td>{'Adword': 'Television', 'Text': 'if i know an...</td>\n",
       "      <td>{'Adword': 'Travel', 'Text': ' itinerary reque...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'Adword': 'Automotive', 'Text': 'cb radio sta...</td>\n",
       "      <td>{'Adword': 'Books and Literature', 'Text': ' m...</td>\n",
       "      <td>{'Adword': 'Business and Finance', 'Text': 'uh...</td>\n",
       "      <td>{'Adword': 'Education', 'Text': ' why don't yo...</td>\n",
       "      <td>{'Adword': 'Events and Attractions', 'Text': '...</td>\n",
       "      <td>{'Adword': 'Family and Relationships', 'Text':...</td>\n",
       "      <td>{'Adword': 'Food &amp; Drink', 'Text': 'different ...</td>\n",
       "      <td>{'Adword': 'Hobbies &amp; Interests', 'Text': '  m...</td>\n",
       "      <td>{'Adword': 'Medical Health', 'Text': ' i want ...</td>\n",
       "      <td>{'Adword': 'Music and Audio', 'Text': 'camera ...</td>\n",
       "      <td>{'Adword': 'Pets', 'Text': 'oh the new tiffany...</td>\n",
       "      <td>{'Adword': 'Real Estate', 'Text': '  and real ...</td>\n",
       "      <td>{'Adword': 'Religion &amp; Spirituality', 'Text': ...</td>\n",
       "      <td>{'Adword': 'Science', 'Text': 'i know a scient...</td>\n",
       "      <td>{'Adword': 'Shopping', 'Text': ' what  do you ...</td>\n",
       "      <td>{'Adword': 'Sports', 'Text': ' what  some next...</td>\n",
       "      <td>{'Adword': 'Style &amp; Fashion', 'Text': ' moans ...</td>\n",
       "      <td>{'Adword': 'Technology &amp; Computing', 'Text': '...</td>\n",
       "      <td>{'Adword': 'Television', 'Text': ' so much of ...</td>\n",
       "      <td>{'Adword': 'Travel', 'Text': '  bucket list ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'Adword': 'Automotive', 'Text': 'it'll have a...</td>\n",
       "      <td>{'Adword': 'Books and Literature', 'Text': ' d...</td>\n",
       "      <td>{'Adword': 'Business and Finance', 'Text': 'i ...</td>\n",
       "      <td>{'Adword': 'Education', 'Text': ' there's boar...</td>\n",
       "      <td>{'Adword': 'Events and Attractions', 'Text': '...</td>\n",
       "      <td>{'Adword': 'Family and Relationships', 'Text':...</td>\n",
       "      <td>{'Adword': 'Food &amp; Drink', 'Text': 'different ...</td>\n",
       "      <td>{'Adword': 'Hobbies &amp; Interests', 'Text': ' so...</td>\n",
       "      <td>{'Adword': 'Medical Health', 'Text': ' your me...</td>\n",
       "      <td>{'Adword': 'Music and Audio', 'Text': '  sighs...</td>\n",
       "      <td>{'Adword': 'Pets', 'Text': 'and the furnishing...</td>\n",
       "      <td>{'Adword': 'Real Estate', 'Text': 'we just got...</td>\n",
       "      <td>{'Adword': 'Religion &amp; Spirituality', 'Text': ...</td>\n",
       "      <td>{'Adword': 'Science', 'Text': 'i'm sort of int...</td>\n",
       "      <td>{'Adword': 'Shopping', 'Text': '  sighs  what ...</td>\n",
       "      <td>{'Adword': 'Sports', 'Text': 'man we are athle...</td>\n",
       "      <td>{'Adword': 'Style &amp; Fashion', 'Text': 'at stak...</td>\n",
       "      <td>{'Adword': 'Technology &amp; Computing', 'Text': '...</td>\n",
       "      <td>{'Adword': 'Television', 'Text': 'the phone in...</td>\n",
       "      <td>{'Adword': 'Travel', 'Text': ' if you find som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'Adword': 'Automotive', 'Text': ' engine 51 t...</td>\n",
       "      <td>{'Adword': 'Books and Literature', 'Text': 'i ...</td>\n",
       "      <td>{'Adword': 'Business and Finance', 'Text': 'wo...</td>\n",
       "      <td>{'Adword': 'Education', 'Text': ' it's your sc...</td>\n",
       "      <td>{'Adword': 'Events and Attractions', 'Text': '...</td>\n",
       "      <td>{'Adword': 'Family and Relationships', 'Text':...</td>\n",
       "      <td>{'Adword': 'Food &amp; Drink', 'Text': 'you must c...</td>\n",
       "      <td>{'Adword': 'Hobbies &amp; Interests', 'Text': 'the...</td>\n",
       "      <td>{'Adword': 'Medical Health', 'Text': ' you peo...</td>\n",
       "      <td>{'Adword': 'Music and Audio', 'Text': 'rock mu...</td>\n",
       "      <td>{'Adword': 'Pets', 'Text': 'i've got some shop...</td>\n",
       "      <td>{'Adword': 'Real Estate', 'Text': ' we sell re...</td>\n",
       "      <td>{'Adword': 'Religion &amp; Spirituality', 'Text': ...</td>\n",
       "      <td>{'Adword': 'Science', 'Text': ' you're the sci...</td>\n",
       "      <td>{'Adword': 'Shopping', 'Text': 'they have gift...</td>\n",
       "      <td>{'Adword': 'Sports', 'Text': 'and we go wrestl...</td>\n",
       "      <td>{'Adword': 'Style &amp; Fashion', 'Text': 'at stak...</td>\n",
       "      <td>{'Adword': 'Technology &amp; Computing', 'Text': '...</td>\n",
       "      <td>{'Adword': 'Television', 'Text': 'lost this se...</td>\n",
       "      <td>{'Adword': 'Travel', 'Text': 'we've got tahiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'Adword': 'Automotive', 'Text': 'alarm sounds...</td>\n",
       "      <td>{'Adword': 'Books and Literature', 'Text': ' h...</td>\n",
       "      <td>{'Adword': 'Business and Finance', 'Text': 'i ...</td>\n",
       "      <td>{'Adword': 'Education', 'Text': ' i'm at belai...</td>\n",
       "      <td>{'Adword': 'Events and Attractions', 'Text': '...</td>\n",
       "      <td>{'Adword': 'Family and Relationships', 'Text':...</td>\n",
       "      <td>{'Adword': 'Food &amp; Drink', 'Text': ' in the cu...</td>\n",
       "      <td>{'Adword': 'Hobbies &amp; Interests', 'Text': ' wh...</td>\n",
       "      <td>{'Adword': 'Medical Health', 'Text': '  and  1...</td>\n",
       "      <td>{'Adword': 'Music and Audio', 'Text': 'somber ...</td>\n",
       "      <td>{'Adword': 'Pets', 'Text': 'so what do you hav...</td>\n",
       "      <td>{'Adword': 'Real Estate', 'Text': 'you can sel...</td>\n",
       "      <td>{'Adword': 'Religion &amp; Spirituality', 'Text': ...</td>\n",
       "      <td>{'Adword': 'Science', 'Text': 'except air and ...</td>\n",
       "      <td>{'Adword': 'Shopping', 'Text': ' like a restoc...</td>\n",
       "      <td>{'Adword': 'Sports', 'Text': 'we got jackets h...</td>\n",
       "      <td>{'Adword': 'Style &amp; Fashion', 'Text': 'at stak...</td>\n",
       "      <td>{'Adword': 'Technology &amp; Computing', 'Text': '...</td>\n",
       "      <td>{'Adword': 'Television', 'Text': '\" al housewi...</td>\n",
       "      <td>{'Adword': 'Travel', 'Text': 'cruise ships are...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Automotive  \\\n",
       "0  {'Adword': 'Automotive', 'Text': 'the most suc...   \n",
       "1  {'Adword': 'Automotive', 'Text': 'cb radio sta...   \n",
       "2  {'Adword': 'Automotive', 'Text': 'it'll have a...   \n",
       "3  {'Adword': 'Automotive', 'Text': ' engine 51 t...   \n",
       "4  {'Adword': 'Automotive', 'Text': 'alarm sounds...   \n",
       "\n",
       "                                Books and Literature  \\\n",
       "0  {'Adword': 'Books and Literature', 'Text': 'fl...   \n",
       "1  {'Adword': 'Books and Literature', 'Text': ' m...   \n",
       "2  {'Adword': 'Books and Literature', 'Text': ' d...   \n",
       "3  {'Adword': 'Books and Literature', 'Text': 'i ...   \n",
       "4  {'Adword': 'Books and Literature', 'Text': ' h...   \n",
       "\n",
       "                                Business and Finance  \\\n",
       "0  {'Adword': 'Business and Finance', 'Text': 'an...   \n",
       "1  {'Adword': 'Business and Finance', 'Text': 'uh...   \n",
       "2  {'Adword': 'Business and Finance', 'Text': 'i ...   \n",
       "3  {'Adword': 'Business and Finance', 'Text': 'wo...   \n",
       "4  {'Adword': 'Business and Finance', 'Text': 'i ...   \n",
       "\n",
       "                                           Education  \\\n",
       "0  {'Adword': 'Education', 'Text': 'the grades th...   \n",
       "1  {'Adword': 'Education', 'Text': ' why don't yo...   \n",
       "2  {'Adword': 'Education', 'Text': ' there's boar...   \n",
       "3  {'Adword': 'Education', 'Text': ' it's your sc...   \n",
       "4  {'Adword': 'Education', 'Text': ' i'm at belai...   \n",
       "\n",
       "                              Events and Attractions  \\\n",
       "0  {'Adword': 'Events and Attractions', 'Text': '...   \n",
       "1  {'Adword': 'Events and Attractions', 'Text': '...   \n",
       "2  {'Adword': 'Events and Attractions', 'Text': '...   \n",
       "3  {'Adword': 'Events and Attractions', 'Text': '...   \n",
       "4  {'Adword': 'Events and Attractions', 'Text': '...   \n",
       "\n",
       "                            Family and Relationships  \\\n",
       "0  {'Adword': 'Family and Relationships', 'Text':...   \n",
       "1  {'Adword': 'Family and Relationships', 'Text':...   \n",
       "2  {'Adword': 'Family and Relationships', 'Text':...   \n",
       "3  {'Adword': 'Family and Relationships', 'Text':...   \n",
       "4  {'Adword': 'Family and Relationships', 'Text':...   \n",
       "\n",
       "                                        Food & Drink  \\\n",
       "0  {'Adword': 'Food & Drink', 'Text': 'friday's e...   \n",
       "1  {'Adword': 'Food & Drink', 'Text': 'different ...   \n",
       "2  {'Adword': 'Food & Drink', 'Text': 'different ...   \n",
       "3  {'Adword': 'Food & Drink', 'Text': 'you must c...   \n",
       "4  {'Adword': 'Food & Drink', 'Text': ' in the cu...   \n",
       "\n",
       "                                 Hobbies & Interests  \\\n",
       "0  {'Adword': 'Hobbies & Interests', 'Text': 'any...   \n",
       "1  {'Adword': 'Hobbies & Interests', 'Text': '  m...   \n",
       "2  {'Adword': 'Hobbies & Interests', 'Text': ' so...   \n",
       "3  {'Adword': 'Hobbies & Interests', 'Text': 'the...   \n",
       "4  {'Adword': 'Hobbies & Interests', 'Text': ' wh...   \n",
       "\n",
       "                                      Medical Health  \\\n",
       "0  {'Adword': 'Medical Health', 'Text': 'there ar...   \n",
       "1  {'Adword': 'Medical Health', 'Text': ' i want ...   \n",
       "2  {'Adword': 'Medical Health', 'Text': ' your me...   \n",
       "3  {'Adword': 'Medical Health', 'Text': ' you peo...   \n",
       "4  {'Adword': 'Medical Health', 'Text': '  and  1...   \n",
       "\n",
       "                                     Music and Audio  \\\n",
       "0  {'Adword': 'Music and Audio', 'Text': 'anythin...   \n",
       "1  {'Adword': 'Music and Audio', 'Text': 'camera ...   \n",
       "2  {'Adword': 'Music and Audio', 'Text': '  sighs...   \n",
       "3  {'Adword': 'Music and Audio', 'Text': 'rock mu...   \n",
       "4  {'Adword': 'Music and Audio', 'Text': 'somber ...   \n",
       "\n",
       "                                                Pets  \\\n",
       "0  {'Adword': 'Pets', 'Text': 'my spare set of ca...   \n",
       "1  {'Adword': 'Pets', 'Text': 'oh the new tiffany...   \n",
       "2  {'Adword': 'Pets', 'Text': 'and the furnishing...   \n",
       "3  {'Adword': 'Pets', 'Text': 'i've got some shop...   \n",
       "4  {'Adword': 'Pets', 'Text': 'so what do you hav...   \n",
       "\n",
       "                                         Real Estate  \\\n",
       "0  {'Adword': 'Real Estate', 'Text': '  as the ma...   \n",
       "1  {'Adword': 'Real Estate', 'Text': '  and real ...   \n",
       "2  {'Adword': 'Real Estate', 'Text': 'we just got...   \n",
       "3  {'Adword': 'Real Estate', 'Text': ' we sell re...   \n",
       "4  {'Adword': 'Real Estate', 'Text': 'you can sel...   \n",
       "\n",
       "                             Religion & Spirituality  \\\n",
       "0  {'Adword': 'Religion & Spirituality', 'Text': ...   \n",
       "1  {'Adword': 'Religion & Spirituality', 'Text': ...   \n",
       "2  {'Adword': 'Religion & Spirituality', 'Text': ...   \n",
       "3  {'Adword': 'Religion & Spirituality', 'Text': ...   \n",
       "4  {'Adword': 'Religion & Spirituality', 'Text': ...   \n",
       "\n",
       "                                             Science  \\\n",
       "0  {'Adword': 'Science', 'Text': 'chandra the wor...   \n",
       "1  {'Adword': 'Science', 'Text': 'i know a scient...   \n",
       "2  {'Adword': 'Science', 'Text': 'i'm sort of int...   \n",
       "3  {'Adword': 'Science', 'Text': ' you're the sci...   \n",
       "4  {'Adword': 'Science', 'Text': 'except air and ...   \n",
       "\n",
       "                                            Shopping  \\\n",
       "0  {'Adword': 'Shopping', 'Text': ' anything in t...   \n",
       "1  {'Adword': 'Shopping', 'Text': ' what  do you ...   \n",
       "2  {'Adword': 'Shopping', 'Text': '  sighs  what ...   \n",
       "3  {'Adword': 'Shopping', 'Text': 'they have gift...   \n",
       "4  {'Adword': 'Shopping', 'Text': ' like a restoc...   \n",
       "\n",
       "                                              Sports  \\\n",
       "0  {'Adword': 'Sports', 'Text': 'talk about a who...   \n",
       "1  {'Adword': 'Sports', 'Text': ' what  some next...   \n",
       "2  {'Adword': 'Sports', 'Text': 'man we are athle...   \n",
       "3  {'Adword': 'Sports', 'Text': 'and we go wrestl...   \n",
       "4  {'Adword': 'Sports', 'Text': 'we got jackets h...   \n",
       "\n",
       "                                     Style & Fashion  \\\n",
       "0  {'Adword': 'Style & Fashion', 'Text': ''s\" fas...   \n",
       "1  {'Adword': 'Style & Fashion', 'Text': ' moans ...   \n",
       "2  {'Adword': 'Style & Fashion', 'Text': 'at stak...   \n",
       "3  {'Adword': 'Style & Fashion', 'Text': 'at stak...   \n",
       "4  {'Adword': 'Style & Fashion', 'Text': 'at stak...   \n",
       "\n",
       "                              Technology & Computing  \\\n",
       "0  {'Adword': 'Technology & Computing', 'Text': '...   \n",
       "1  {'Adword': 'Technology & Computing', 'Text': '...   \n",
       "2  {'Adword': 'Technology & Computing', 'Text': '...   \n",
       "3  {'Adword': 'Technology & Computing', 'Text': '...   \n",
       "4  {'Adword': 'Technology & Computing', 'Text': '...   \n",
       "\n",
       "                                          Television  \\\n",
       "0  {'Adword': 'Television', 'Text': 'if i know an...   \n",
       "1  {'Adword': 'Television', 'Text': ' so much of ...   \n",
       "2  {'Adword': 'Television', 'Text': 'the phone in...   \n",
       "3  {'Adword': 'Television', 'Text': 'lost this se...   \n",
       "4  {'Adword': 'Television', 'Text': '\" al housewi...   \n",
       "\n",
       "                                              Travel  \n",
       "0  {'Adword': 'Travel', 'Text': ' itinerary reque...  \n",
       "1  {'Adword': 'Travel', 'Text': '  bucket list ch...  \n",
       "2  {'Adword': 'Travel', 'Text': ' if you find som...  \n",
       "3  {'Adword': 'Travel', 'Text': 'we've got tahiti...  \n",
       "4  {'Adword': 'Travel', 'Text': 'cruise ships are...  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df11.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11.to_csv(\"Top20Sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f8fa3e926face00369008f823e0a800506eaa9d6c726ba0fbe6c8d0f6896eaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
